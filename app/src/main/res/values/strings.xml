<resources>
    <string name="app_name">ComputerArchitecture</string>
    <string name="multithreading">Multithreading</string>
    <string name="computer_architecture">Computer Architecture</string>
    <string name="static_scheduling_name">Static Scheduling</string>
    <string name="static_scheduling_description">The Compiler can attempt to schedule instructions. </string>
    <string name="dynamic_scheduling_name">Dynamic scheduling</string>
    <string name="dynamic_scheduling_description">The Hardware rearranges the instruction execution to reduce the
        stalls.</string>
    <string name="as_soon_as_possible_name">As soon as possible (ASAP)</string>
    <string name="as_soon_as_possible_description">Executes instructions as soon as possible.</string>
    <string name="as_late_as_possible_name">As late as possible(ALAP)</string>
    <string name="as_late_as_possible_description">Executes instructions as late as possible.</string>
    <string name="cgmt_description">Long stalls are partially hidden by switching to another thread that uses the
        resources of the processor. This switching reduces the number of completely idle clock cycles. In a
        coarse-grained multithreaded processor, however, thread switching only occurs when there is a stall.
        Because the new thread has a start-up period, there are likely to be some fully idle cycles remaining.</string>
    <string name="fgmt_description">The interleaving of threads can eliminate fully empty slots. In addition, because
        the issuing thread is changed on every clock cycle, longer latency operations can be hidden. Because
        instruction issue and execution are connected, a thread can only issue as many instructions as are ready.
        With a narrow issue width this is not a problem (a cycle is either occupied or not), which is why fine-grained
        multithreading works perfectly for a single issue processor, and SMT would make no sense.</string>
    <string name="smt_description">If one implements fine-grained threading on top of a multiple-issue dynamically
        schedule processor, the result is SMT. In all existing SMT implementations, all issues come from one thread,
        although instructions from different threads can initiate execution in the same cycle, using the dynamic
        scheduling hardware to determine what instructions are ready. Simultaneous multithreading uses the insight
        that a dynamically scheduled processor already has many of the hardware mechanisms needed to support the
        mechanism, including a large virtual register set.</string>
    <string name="simultanious_multithreading">Simultaneous Multithreading</string>
    <string name="fine_grained_multithreading">Fine-grained multithreading</string>
    <string name="coarse_grained_multithreading">Coarse-grained Multithreading</string>
    <string name="hardware_layer">Hardware Layer</string>
    <string name="software_layer">Software Layer</string>
    <string name="speedup">Speedup</string>
    <string name="speedup_exercise_description">
        A sequential program can be divided into 5 parts A to E, which must be
    executed in this order due to their dependencies. The table lists the amount of run-time each part contributes to
    the run-time of the program. Parts A, C and E cannot be parallelized. Part B can be transformed in max. 16
        sub-parts that can be executed in parallel. For the parallel execution of part D, no restrictions exist.</string>

    <string name="part">Part</string>
    <string name="multiprocessor_systems">Multiprocessor Systems</string>
    <string name="numa_title_short">NUMA</string>
    <string name="numa_title_long">Non-Uniform-Memory-Access</string>
    <string name="numa_runtime_description">"Memory access time depends on the memory location relative to the processor. Memory may be local, one hop away, or two hops away. "</string>
    <string name="gpus">Graphics Processing Units</string>
    <string name="open_cl">OpenCL</string>
    <string name="posix_threads">POSIX Threads</string>
    <string name="includes">Includes</string>
    <string name="pthread_include"><![CDATA[#include <pthread.h>]]></string>
    <string name="thread_creation">Thread Creation</string>
    <string name="pthread_create">pthread_create(thread, attr, start_routine, arg)</string>
    <string name="thread">thread</string>
    <string name="attr">attr</string>
    <string name="start_routine">start_routine</string>
    <string name="arg">arg</string>
    <string name="pthread_create_description">Creates a new thread and makes it executable. </string>
    <string name="thread_description">An opaque, unique identifier for the new thread returned by the
        subroutine.</string>
    <string name="attr_description">An opaque attribute object that may be used to set thread attributes.
        You can specify a thread attributes object, or NULL for the default values.</string>
    <string name="start_routine_description">The C routine that the thread will execute once it is created.</string>
    <string name="arg_description">A single argument that may be passed to start_routine.
        It must be passed by reference as (void *).NULL may be used if no argument is to be passed.</string>
    <string name="caching">Caching</string>
    <string name="average_runtimes">Average Runtimes</string>
    <string name="block_placement">Block Placement</string>
    <string name="block_identification">Block Identification</string>
    <string name="block_replacement">Block Replacement</string>
    <string name="write_strategy">Write Strategy</string>
    <string name="block_placement_question">Where can a memory block be stored in the cache?</string>
    <string name="navigate_to">Navigate to</string>
    <string name="block_identification_question">How is a memory block found?</string>
    <string name="block_replacement_question">Which block should be replaced in the event of a failed access (cache miss)?</string>
    <string name="write_strategy_question">What happens when data is written to the cache?</string>
    <string name="direct_mapping_block_placement">Each block can be placed at exactly one location in the cache.</string>
    <string name="direct_mapping">Direct Mapping</string>
    <string name="set_associative_mapping">Set Associative Mapping</string>
    <string name="set_associative_mapping_block_placement">Each block can be placed at a restricted set of locations in the cache.</string>
    <string name="associative_mapping">Associative Mapping</string>
    <string name="associative_mapping_block_placement">Each block can be placed at any location in the cache.</string>
    <string name="tag">Tag</string>
    <string name="tag_description">The tag contains the most significant bits of the address, which are checked against
        all rows in the current set to see if this set contains the requested
        address. </string>
    <string name="tag_length"> tag_length = address_length - index_length - offset_length</string>
    <string name="index">Index</string>
    <string name="index_description">The index describes which cache set that the data has been put in.</string>
    <string name="index_length"> index_length = ceil(log_2(s)), for s cache sets </string>
    <string name="offset">Offset</string>
    <string name="offset_description">The block offset specifies the desired data within the stored data block within
        the cache row.</string>
    <string name="offset_length">offset_length = ceil(log_2(b)), for b bytes per block </string>
    <string name="direct_mapping_block_identification">Tag, index and offset</string>
    <string name="set_associative_mapping_block_identification">Tag, index and offset  </string>
    <string name="associative_mapping_block_identification">Tag and index</string>

</resources>